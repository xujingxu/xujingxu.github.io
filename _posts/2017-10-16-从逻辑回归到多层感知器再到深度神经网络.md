---
layout: post
title: "从逻辑回归到多层感知器再到深度神经网络"
date: 2017-10-16 
description: "logistic regression, MLP, DNN"
tag: Deep learning 
---   
  听说过很多次玻尔兹曼机，限制性玻尔兹曼机（RBM），深度玻尔兹曼机（DBM）,深度置信网络（DBM）,以及感知机（perceptron），多层感知器（MLP），深度神经网络（DNN）,
在接下来的2篇博客中会详细阐述它们之前的演化及关系，在这篇博客中主要讲述感知器，多层感知器（MLP），深度神经网络（DNN）。

## 程序说明
  
  首先我们通过程序去理解MLP和logistic regression之间的关系，我们同样使用TensorFlow和keras来演示。如果没有安装的话，
  请参考前面的文章[文章链接](http://xujingxu.cn/2017/10/ubuntu14.04%E9%85%8D%E7%BD%AEtensorflow%E5%92%8Ckeras/)

	$ import numpy as np
	$ from sklearn.datasets import make_circles
	$ from keras.layers import Input, Dense
	$ from keras.models import Sequential
	$ X, y = make_circles(n_samples=5000, factor=.3, noise=.05)
	$ X_train = X[:4000]
	$ y_train = y[:4000]
	$ X_val = X[4000:]
	$ y_val = y[4000:]
	$ num_variables = X.shape[1]
	
	$ logreg = Sequential()
	$ logreg.add(Dense(output_dim=1, input_dim=num_variables, activation='sigmoid'))
	
	$ logreg.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
	$ logreg.fit(X_train, y_train,validation_data=[X_val,y_val],verbose=0)
	$ val_score = logreg.evaluate(X_val,y_val,verbose=0)
  
   上面是一个logistic regression实现分类的例子。logistic回归是一种概率的线性分类器。它是参数化的权重矩阵W和偏置向量B分类是将输入向量到一套超平面，每个点对应一个类。从输入到超平面的距离反映了输入是对应类的成员的概率。
   
 	$ num_hidden = 128
 	$ mlp = Sequential()
	$ mlp.add(Dense(output_dim=num_hidden, input_dim=num_variables, activation='relu'))
	% mlp.add(Dense(output_dim=num_hidden, input_dim=numhidden, activation='relu'))
	% mlp.add(Dense(output_dim=1, activation='sigmoid'))
	% mlp.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
	% mlp.fit(X_train, y_train,validation_data=[X_val,y_val],nb_epoch=10,verbose=1)
	
  这是一个使用多层感知器实现分类的例子，MLP可以被看作是一个logistic回归分类器，其中输入首先使用非线性变换φ进行转换。这个转换将输入数据投射到一个空间中，它变成线性可分的空间。这个中间层称为隐藏层。单隐层足以使MLP的通用逼近。然而，我们稍后会看到，使用许多这样的隐藏层有巨大的好处，也就是深层学习的前提。
  
  如果我们加入足够的隐藏层，这样就可以看成深度神经网络（DNN）了。
	
	
  

	
	
  
	
  
  
